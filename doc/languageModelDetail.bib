@article{miller1995wordnet,
	title = {{WordNet:} A Lexical Database for English},
	volume = {38},
	number = {11},
	journal = {Communications of the {ACM}},
	author = {G. A. Miller},
	year = {1995},
	pages = {39--41}
},

@article{shannon1948mathematical,
	title = {A mathematical theory of communication},
	volume = {27},
	issn = {1559-1662},
	journal = {Bell System Technical Journal},
	author = {C. E Shannon},
	year = {1948},
	pages = {379---423}
},

@article{chen2002survey,
	title = {A survey of smoothing techniques for {ME} models},
	volume = {8},
	issn = {1063-6676},
	number = {1},
	journal = {Speech and Audio Processing, {IEEE} Transactions on},
	author = {S. F Chen and R. Rosenfeld},
	year = {2002},
	keywords = {language model, maximum entropy, to annotate, to read},
	pages = {37--50}
},

@techreport{goodman2001bit,
	title = {A Bit of Progress in Language Modeling},
	number = {{MSR-TR-2001-72}},
	institution = {Microsoft},
	author = {J. T Goodman},
	year = {2001},
	keywords = {evaluation, language model, to annotate, to read}
},

@article{berger1996maximum,
	title = {A maximum entropy approach to natural language processing},
	volume = {22},
	number = {1},
	journal = {Computational linguistics},
	author = {A. L Berger and V. {J.D} Pietra and S. {A.D} Pietra},
	year = {1996},
	keywords = {machine learning, maximum entropy, to annotate, to read, translation},
	pages = {39--71}
},

@article{roark2001robust,
	title = {Robust probabilistic predictive syntactic processing},
	journal = {{PhD} Thesis, Arxiv preprint cs/0105019},
	author = {B. Roark},
	year = {2001},
	keywords = {language model, parsing, to read}
},

@inproceedings{roark1999efficient,
	title = {Efficient probabilistic top-down and left-corner parsing},
	booktitle = {Proceedings of the 37th annual meeting of the Association for Computational Linguistics on Computational Linguistics},
	author = {B. Roark and M. Johnson},
	year = {1999},
	keywords = {language model, parsing},
	pages = {421--428},
	annote = {{{\textless}p{\textgreater}Seems} to describe a heuristic search strategy that should be faster than the dynamic programming approach, yet is still competitive.  Results are promising, but their methods are not evaluated with respect to other state-of-the-art parsers.{\textless}/p{\textgreater}}
},

@article{teh2006bayesian,
	title = {A Bayesian interpretation of interpolated {Kneser-Ney}},
	journal = {School of Computing, National University of Singapore, Tech. Rep. {TRA2/06}},
	author = {Y. W Teh},
	year = {2006}
},

@inproceedings{kneser1995improved,
	title = {Improved backing-off for m-gram language modeling},
	volume = {1},
	isbn = {0780324315},
	booktitle = {Acoustics, Speech, and Signal Processing, 1995. {ICASSP-95.,} 1995 International Conference on},
	author = {R. Kneser and H. Ney},
	year = {1995},
	pages = {181--184}
},

@article{katz1987estimation,
	title = {Estimation of probabilities from sparse data for the language model component of a speech recognizer},
	volume = {35},
	number = {3},
	journal = {{IEEE} {TRANSACTIONS} {ON} {ACOUSTICS,} {SPEECH,} {AND} {SIGNAL} {PROCESSING}},
	author = {Slava M. Katz},
	year = {1987}
},

@techreport{chen1998empirical,
	title = {An empirical study of smoothing techniques for language modeling},
	number = {{TR-10-98}},
	institution = {Harvard},
	author = {S. Chen and J. Goodman},
	year = {1998}
},
A longer version of chen1999 empirical

@article{nadas1985turings,
	title = {On Turing¿s Formula for Word Probabilities},
	volume = {{ASSP-33}},
	number = {6},
	journal = {{IEEE} {TRANSACTIONS} {ON} {ACOUSTICS,} {SPEECH,} {AND} {SIGNAL} {PROCESSING}},
	author = {Arthur Nadas},
	year = {1985},
},

@article{good1953population,
	title = {The population frequencies of species and the estimation of population parameters},
	volume = {40},
	number = {3},
	journal = {Biometrika},
	author = {I. J Good},
	year = {1953},
	pages = {237¿264}
},

@book{manning2003foundations,
	address = {Cambridge, {MA,} {USA}},
	edition = {6},
	title = {Foundations of statistical natural language processing},
	isbn = {0-262-13360-1},
	publisher = {{MIT} Press},
	author = {Christopher D Manning and Hinrich SchÃ¼tze},
	year = {2003}
},

@book{jurafsky2008speech,
	edition = {2},
	title = {Speech and Language Processing},
	isbn = {978-0131873216},
	author = {D. Jurafsky and J. Martin},
	year = {2008}
},

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Given Articles
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
@article{roy2005towards,
	title = {Towards situated speech understanding: Visual context priming of language models},
	volume = {19},
	issn = {0885-2308},
	number = {2},
	journal = {Computer Speech \& Language},
	author = {D. Roy and N. Mukherjee},
	year = {2005},
	pages = {227â€“248}
},

@article{roark2001probabilistic,
	title = {Probabilistic top-down parsing and language modeling},
	volume = {27},
	issn = {0891-2017},
	number = {2},
	journal = {Computational Linguistics},
	author = {B. Roark},
	year = {2001},
	pages = {249â€“276}
},

@article{chen1999empirical,
	title = {An Empirical Study of Smoothing Techniques for Language Modeling},
	volume = {13},
	number = {4},
	journal = {Computer Speech and Language},
	author = {Stanley F. Chen and Joshua T. Goodman},
	year = {1999},
	pages = {359---393}
},

@article{rosenfeld1995maximum,
	title = {A Maximum Entropy Approach to Adaptive Statistical Language Modeling},
	volume = {10},
	abstract = {An adaptive statistical language model is described, which successfully integrates long distance linguistic information with other knowledge sources. Most existing statistical language models exploit only the immediate history of a text. To extract information from further back in the documentâ€™s history, we propose and use trigger pairs as the basic information bearing elements. This allows the model to adapt its expectations to the topic of discourse. Next, statistical evidence from multiple sources must be combined. Traditionally, linear interpolation and its variants have been used, but these are shown here to be seriously deficient. Instead, we apply the principle of Maximum Entropy {(ME).} Each information source gives rise to a set of constraints, to be imposed on the combined estimate. The intersection of these constraints is the set of probability functions which are consistent with all the information sources. The function with the highest entropy within that set is the {ME} solution. Given consistent statistical evidence, a unique {ME} solution is guaranteed to exist, and an iterative algorithm exists which is guaranteed to converge to it. The {ME} framework is extremely general: any phenomenon that can be described in terms of statistics of the text can be readily incorporated. An adaptive language model based on the {ME} approach was trained on the Wall Street Journal corpus, and showed 32\%â€“39\% perplexity reduction over the baseline. When interfaced to {SPHINX-II,} Carnegie Mellonâ€™s speech recognizer, it reduced its error rate by 10\%â€“14\%. This thus illustrates the feasibility of incorporating many diverse knowledge sources in a single, unified statistical framework.},
	journal = {Computer Speech and Language},
	author = {Ronald Rosenfeld},
	year = {1996},
	pages = {187---228}
},

@article{brown1992class,
	title = {Class-based n-gram models of natural language},
	volume = {18},
	issn = {0891-2017},
	number = {4},
	journal = {Computational linguistics},
	author = {P. F Brown and P. V Desouza and R. L Mercer and V. {J.D} Pietra and J. C Lai},
	year = {1992},
	pages = {467â€“479}
}

